{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07e1340f",
   "metadata": {},
   "source": [
    "<h1> Pruebas para Liga de Bolsa </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27cba82b",
   "metadata": {},
   "source": [
    "<h2> Descargar información de las cotizaciones </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4f74eb",
   "metadata": {},
   "source": [
    "Se guardarán la información de la cotización en un dataframe de pequeñas dimensiones y con la mínima información posible para ahorrar memoria."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0e7fef",
   "metadata": {},
   "source": [
    "### Próximos pasos\n",
    "\n",
    "- Mostrar como funciona el análisis bursátil a nivel de código.\n",
    "\n",
    "- Probar a ejecutar el Jupyter Notebook en una máquina virtual de Google Cloud\n",
    "\n",
    "> Buscar algún tutorial para ello e intentar averiguar la manera de seleccionar la VM más adecuada.\n",
    "\n",
    "- Probar la API de Interactive Brokers (trading intradía)\n",
    "\n",
    "> Comprobar compatibilidad con Google Cloud\n",
    "\n",
    "> Buscar alternativas si procede\n",
    "\n",
    "- Desarrollar un algoritmo mediante indicadores técnicos.\n",
    "\n",
    "> Comprobar significancia de los indicadores y otros valores (por ejemplo, la beta).\n",
    "\n",
    "> Utilizar métodos de valididación cruzada.\n",
    "\n",
    "> ¿Seleccionar acciones por las señales de compra y/o por sus valores de beta?\n",
    "\n",
    "> Tanto en este paso como en los posteriores, será interesante realizar un análisis de de componentes principales para optimizar el código.\n",
    "\n",
    "- Del algortimo anterior, integrar indicadores fundamentales\n",
    "\n",
    "> Asegurarse de que estos indicadores fundamentales tengan valores concordes a su periodo.\n",
    "\n",
    "> Comprobar si aportarán en el análisis.\n",
    "\n",
    "- Teniendo en cuenta lo de antes, integrar el machine learning\n",
    "\n",
    "> Tal vez un clasificador que determine, con esperanza estadística positiva, si la compra o venta serán rentables o no.\n",
    "\n",
    "- Con todo lo de antes, desarrollar un algoritmo de IA que tenga en cuenta los máximos parámetros posibles.\n",
    "\n",
    "> Pueden combinarse con análisis de sentimiento de cara a un activo (creo que puede descargarse por internet)\n",
    "\n",
    "> ¿Desarrollar una IA para cada tipo de acción (cuidado con el overfitting)? ¿O para cada tipo de mercado?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9be66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install investpy package. This first command does not work.\n",
    "# ! pip install git+https://github.com/alvarobartt/investpy.git@master\n",
    "# This one works!\n",
    "# ! pip install investpy\n",
    "\n",
    "# Interactive Brokers API\n",
    "# ! pip install ibapi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a2fa56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import investpy\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a3b664",
   "metadata": {},
   "source": [
    "### Investing API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0252db01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load some historical data from the AAPL stock. Date format is: dd/mm/yyyy\n",
    "    # Frequency: daily\n",
    "    # Open, maximum, minimum and close prices, volume and currency.\n",
    "aapl = investpy.get_stock_historical_data(stock='AAPL', country='United States', from_date='01/01/2020', to_date='30/10/2021')\n",
    "aapl\n",
    "\n",
    "# Get the five first values of volume from the df dataframe\n",
    "# with these example options:\n",
    "# 1. df.iloc[0:5][\"Volume\"] \n",
    "# 2. df.head()[\"Volume\"]\n",
    "\n",
    "# The same, but with the last five values:\n",
    "# 1. df.iloc[-5:][\"Volume\"]\n",
    "# 2. df.tail()[\"Volume\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17dbbe39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load futher data from a stock\n",
    "\n",
    "# Apple stock will be saved at search_result\n",
    "aapl_info = investpy.search_quotes(text='apple', products=['stocks'], countries=['united states'], n_results=1)\n",
    "# Save the information in a dict variable\n",
    "aapl_info = aapl_info.retrieve_information()\n",
    "print(aapl_info)\n",
    "\n",
    "# Technical information:\n",
    "    # prevClose, dailyRange, open (most recent), weekRange (most recent), volume(unkwown) avgVolume(it could be weekly)\n",
    "# Fundamental information:\n",
    "    # revenue, eps (earning per share), marketCap, dividend (last), ratio (P/E).\n",
    "# Other or both\n",
    "    # beta, oneYearReturn, sharesOutstanding, nextEarningDate\n",
    "\n",
    "'''\n",
    "# avgVolume: 74388303\n",
    "\n",
    "for x in range(1, len(df[\"Volume\"])):\n",
    "    print(f\"{x}: {df.tail(x)['Volume'].mean()}\")\n",
    "    # mean output 29 is similar\n",
    "\n",
    "for x in range(1, len(df[\"Volume\"])):\n",
    "    print(f\"{x}: {df.tail(x)['Volume'].median()}\")\n",
    "    # median output 30 is similar\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab03671c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import investpy\n",
    "\n",
    "# s_results = investpy.search_quotes(text='a', products=['stocks'], countries=['united states'], n_results=10)\n",
    "\n",
    "# s_results = map(lambda x: print(x), s_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9c17a7",
   "metadata": {},
   "source": [
    "### Interactive Brokers API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f3f660c",
   "metadata": {},
   "source": [
    "It will allow us to work with intraday data.\n",
    "- IB Gateway is mandatory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1c5922",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ibapi.client import EClient\n",
    "from ibapi.wrapper import EWrapper\n",
    "from ibapi.contract import Contract\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "\n",
    "class IBapi(EWrapper, EClient):\n",
    "    def __init__(self):\n",
    "        EClient.__init__(self,self)\n",
    "        cols = ['date', 'open', 'high', 'low', 'close']\n",
    "        self.df = pd.DataFrame(columns=cols)\n",
    "    \n",
    "    def historicalData(self, reqId, bar):\n",
    "        print(\" Date:\", bar.date, \"Open:\", bar.open, \"High:\", bar.high, \"Low:\", bar.low, \"Close:\", bar.close) #, \"Volume: \", bar.volume, \"Count: \", bar.barCount)\n",
    "        dftemp = pd.DataFrame({'date':bar.date,'open':bar.open,'high':bar.high,'low':bar.low, 'close':bar.close}, index=[0])\n",
    "        self.df = pd.concat([self.df, dftemp], axis=0)\n",
    "        \n",
    "    def historicalDataEnd(self, reqId: int, start: str, end: str):\n",
    "        super().historicalDataEnd(reqId, start, end)\n",
    "        print(\"HistoricalDataEnd. ReqId:\", reqId, \"from\", start, \"to\", end)\n",
    "        self.df.to_csv(\"GBP_USD_1Y_15mins.csv\",index=False)\n",
    "        self.disconnect()\n",
    "\n",
    "app = IBapi()\n",
    "app.connect('127.0.0.1', 4002, 0)\n",
    "\n",
    "#Create contract object\n",
    "def defineContract(symbol,secType,exchange,currency='USD'):\n",
    "    contract = Contract()\n",
    "    contract.symbol = symbol\n",
    "    contract.secType = secType\n",
    "    contract.exchange = exchange\n",
    "    contract.currency = currency\n",
    "    return contract\n",
    "\n",
    "contract = defineContract(symbol='GBP',secType='CASH',exchange='IDEALPRO')\n",
    "queryTime = (datetime.today() - timedelta(days=30)).strftime(\"%Y%m%d %H:%M:%S\")\n",
    "#queryTime = \"\"\n",
    "duration = '1 Y'\n",
    "barsize = '15 mins'\n",
    "priceType = 'MIDPOINT'\n",
    "\n",
    "app.reqHistoricalData(1, contract, queryTime, duration, barsize, priceType, 1, 1, False, [])\n",
    "app.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77b3d83",
   "metadata": {},
   "source": [
    "## Technical analysis\n",
    "Futher information [here](https://medium.com/codex/this-python-library-will-help-you-get-stock-technical-indicators-in-one-line-of-code-c11ed2c8e45f) (stockstats) and [here](https://towardsdatascience.com/technical-analysis-library-to-financial-datasets-with-pandas-python-4b2b390d3543) (ta)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a0fe30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip3 install stockstats\n",
    "# ! pip3 install ta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361645f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Technical analysis libraries\n",
    "from stockstats import StockDataFrame\n",
    "import ta\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "487830b8",
   "metadata": {},
   "source": [
    "### stockstats library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13dce6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datafrate to Stockstats dataframe library\n",
    "aapl = StockDataFrame(aapl)\n",
    "aapl.columns = aapl.columns.str.lower()\n",
    "aapl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e9f62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostrar tres medias móviles (de 10, 20 y 50 periodos), RSI (14 periodos) and MACD\n",
    "aapl[['close_10_sma', 'close_20_sma', 'close_50_sma', 'rsi_14', 'macd', 'macds']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33116a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot size in inches\n",
    "plt.rcParams['figure.figsize'] = [10, 5]\n",
    "\n",
    "# Plot the last 90 periods\n",
    "plt.plot(aapl.iloc[-90:]['close'], linewidth = 2, label = 'AAPL')\n",
    "plt.plot(aapl.iloc[-90:]['close_10_sma'], linewidth = 2, alpha = 0.6, label = 'SMA 10')\n",
    "plt.plot(aapl.iloc[-90:]['close_20_sma'], linewidth = 2, alpha = 0.6, label = 'SMA 20')\n",
    "plt.plot(aapl.iloc[-90:]['close_50_sma'], linewidth = 2, alpha = 0.6, label = 'SMA 50')\n",
    "plt.legend(loc = 'upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63bfd50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "aapl['close_50_sma_xd_close_20_sma']\n",
    "\n",
    "# Detect where the signals cross each other.\n",
    "# Create buy and sell signals by close price, SMA_50 and SMA_20.\n",
    "# This could be interesting for testing methods \n",
    "buy_signals = aapl['close_50_sma_xd_close_20_sma']\n",
    "sell_signals = aapl['close_20_sma_xd_close_50_sma']\n",
    "\n",
    "for i in range(len(buy_signals)):\n",
    "    if buy_signals.iloc[i] == True:\n",
    "        buy_signals.iloc[i] = aapl.close[i]\n",
    "    else:\n",
    "        buy_signals.iloc[i] = np.nan\n",
    "\n",
    "for i in range(len(sell_signals)):    \n",
    "    if sell_signals.iloc[i] == True:\n",
    "        sell_signals.iloc[i] = aapl.close[i]\n",
    "    else:\n",
    "        sell_signals.iloc[i] = np.nan\n",
    "\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [10, 5]\n",
    "\n",
    "# Plot stock, indicators and signals\n",
    "plt.plot(aapl['close'], linewidth = 2.5, label = 'AAPL')\n",
    "plt.plot(aapl['close_20_sma'], linewidth = 2.5, alpha = 0.6, label = 'SMA 20')\n",
    "plt.plot(aapl['close_50_sma'], linewidth = 2.5, alpha = 0.6, label = 'SMA 50')\n",
    "plt.plot(aapl.index, buy_signals, marker = '^', markersize = 10, color = 'green', linewidth = 0, label = 'BUY SIGNAL')\n",
    "plt.plot(aapl.index, sell_signals, marker = 'v', markersize = 10, color = 'r', linewidth = 0, label = 'SELL SIGNAL')\n",
    "plt.legend(loc = 'upper left')\n",
    "plt.title('AAPL SMA 20,50 CROSSOVER STRATEGY SIGNALS')\n",
    "plt.style.use('bmh')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be42a6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIXME: MACD must be plotted in differnt subplot.\n",
    "# Also, it is necessary to check the different indicators, apart of MACD and MACDS.\n",
    "# Maybe, it can be interesting look for some different styles.\n",
    "\n",
    "buy_signals = aapl['macds_xd_macd']\n",
    "sell_signals = aapl['macd_xd_macds']\n",
    "\n",
    "for i in range(len(buy_signals)):\n",
    "    if buy_signals.iloc[i] == True:\n",
    "        buy_signals.iloc[i] = aapl.close[i]\n",
    "    else:\n",
    "        buy_signals.iloc[i] = np.nan\n",
    "\n",
    "for i in range(len(sell_signals)):    \n",
    "    if sell_signals.iloc[i] == True:\n",
    "        sell_signals.iloc[i] = aapl.close[i]\n",
    "    else:\n",
    "        sell_signals.iloc[i] = np.nan\n",
    "\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [10, 5]\n",
    "\n",
    "# Plot stock, indicators and signals\n",
    "plt.plot(aapl['close'], linewidth = 2.5, label = 'AAPL')\n",
    "plt.plot(aapl['macd'], linewidth = 2.5, alpha = 0.6, label = 'MACD')\n",
    "plt.plot(aapl['macds'], linewidth = 2.5, alpha = 0.6, label = 'MACD_SIGNAL')\n",
    "plt.plot(aapl.index, buy_signals, marker = '^', markersize = 10, color = 'green', linewidth = 0, label = 'BUY SIGNAL')\n",
    "plt.plot(aapl.index, sell_signals, marker = 'v', markersize = 10, color = 'r', linewidth = 0, label = 'SELL SIGNAL')\n",
    "\n",
    "plt.legend(loc = 'upper left')\n",
    "plt.title('AAPL MACD CROSSOVER STRATEGY SIGNALS')\n",
    "plt.style.use('bmh')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143025ff",
   "metadata": {},
   "source": [
    "### ta library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba4daee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e162d6b1",
   "metadata": {},
   "source": [
    "## Artificial Intelligence example\n",
    "\n",
    "Example [here](https://www.thepythoncode.com/article/stock-price-prediction-in-python-using-tensorflow-2-and-keras)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ad7e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip3 install tensorflow pandas numpy matplotlib yahoo_fin sklearn\n",
    "# ! pip3 install yahoo_fin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc36ebc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from yahoo_fin import stock_info as si\n",
    "from collections import deque\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36045cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seed, so we can get the same results after rerunning several times\n",
    "np.random.seed(206)\n",
    "tf.random.set_seed(206)\n",
    "random.seed(206)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b0d5c3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Used to correct an error with model.add()\n",
    "! pip install numpy==1.18.5 --user"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69fdf1f2",
   "metadata": {},
   "source": [
    "### Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be7902a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_in_unison(a, b):\n",
    "    # shuffle two arrays in the same way\n",
    "    state = np.random.get_state()\n",
    "    np.random.shuffle(a)\n",
    "    np.random.set_state(state)\n",
    "    np.random.shuffle(b)\n",
    "\n",
    "def load_data(ticker, n_steps=50, scale=True, shuffle=True, lookup_step=1, split_by_date=True,\n",
    "                test_size=0.2, feature_columns=['adjclose', 'volume', 'open', 'high', 'low']):\n",
    "    \"\"\"\n",
    "    Loads data from Yahoo Finance source, as well as scaling, shuffling, normalizing and splitting.\n",
    "    Params:\n",
    "        ticker (str/pd.DataFrame): the ticker you want to load, examples include AAPL, TESL, etc.\n",
    "        n_steps (int): the historical sequence length (i.e window size) used to predict, default is 50\n",
    "        scale (bool): whether to scale prices from 0 to 1, default is True\n",
    "        shuffle (bool): whether to shuffle the dataset (both training & testing), default is True\n",
    "        lookup_step (int): the future lookup step to predict, default is 1 (e.g next day)\n",
    "        split_by_date (bool): whether we split the dataset into training/testing by date, setting it \n",
    "            to False will split datasets in a random way\n",
    "        test_size (float): ratio for test data, default is 0.2 (20% testing data)\n",
    "        feature_columns (list): the list of features to use to feed into the model, default is everything grabbed from yahoo_fin\n",
    "    \"\"\"\n",
    "    # see if ticker is already a loaded stock from yahoo finance\n",
    "    if isinstance(ticker, str):\n",
    "        # load it from yahoo_fin library\n",
    "        df = si.get_data(ticker)\n",
    "    elif isinstance(ticker, pd.DataFrame):\n",
    "        # already loaded, use it directly\n",
    "        df = ticker\n",
    "    else:\n",
    "        raise TypeError(\"ticker can be either a str or a `pd.DataFrame` instances\")\n",
    "    # this will contain all the elements we want to return from this function\n",
    "    result = {}\n",
    "    # we will also return the original dataframe itself\n",
    "    result['df'] = df.copy()\n",
    "    # make sure that the passed feature_columns exist in the dataframe\n",
    "    for col in feature_columns:\n",
    "        assert col in df.columns, f\"'{col}' does not exist in the dataframe.\"\n",
    "    # add date as a column\n",
    "    if \"date\" not in df.columns:\n",
    "        df[\"date\"] = df.index\n",
    "    if scale:\n",
    "        column_scaler = {}\n",
    "        # scale the data (prices) from 0 to 1\n",
    "        for column in feature_columns:\n",
    "            scaler = preprocessing.MinMaxScaler()\n",
    "            df[column] = scaler.fit_transform(np.expand_dims(df[column].values, axis=1))\n",
    "            column_scaler[column] = scaler\n",
    "        # add the MinMaxScaler instances to the result returned\n",
    "        result[\"column_scaler\"] = column_scaler\n",
    "    # add the target column (label) by shifting by `lookup_step`\n",
    "    df['future'] = df['adjclose'].shift(-lookup_step)\n",
    "    # last `lookup_step` columns contains NaN in future column\n",
    "    # get them before droping NaNs\n",
    "    last_sequence = np.array(df[feature_columns].tail(lookup_step))\n",
    "    # drop NaNs\n",
    "    df.dropna(inplace=True)\n",
    "    sequence_data = []\n",
    "    sequences = deque(maxlen=n_steps)\n",
    "    for entry, target in zip(df[feature_columns + [\"date\"]].values, df['future'].values):\n",
    "        sequences.append(entry)\n",
    "        if len(sequences) == n_steps:\n",
    "            sequence_data.append([np.array(sequences), target])\n",
    "    # get the last sequence by appending the last `n_step` sequence with `lookup_step` sequence\n",
    "    # for instance, if n_steps=50 and lookup_step=10, last_sequence should be of 60 (that is 50+10) length\n",
    "    # this last_sequence will be used to predict future stock prices that are not available in the dataset\n",
    "    last_sequence = list([s[:len(feature_columns)] for s in sequences]) + list(last_sequence)\n",
    "    last_sequence = np.array(last_sequence).astype(np.float32)\n",
    "    # add to result\n",
    "    result['last_sequence'] = last_sequence\n",
    "    # construct the X's and y's\n",
    "    X, y = [], []\n",
    "    for seq, target in sequence_data:\n",
    "        X.append(seq)\n",
    "        y.append(target)\n",
    "    # convert to numpy arrays\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    if split_by_date:\n",
    "        # split the dataset into training & testing sets by date (not randomly splitting)\n",
    "        train_samples = int((1 - test_size) * len(X))\n",
    "        result[\"X_train\"] = X[:train_samples]\n",
    "        result[\"y_train\"] = y[:train_samples]\n",
    "        result[\"X_test\"]  = X[train_samples:]\n",
    "        result[\"y_test\"]  = y[train_samples:]\n",
    "        if shuffle:\n",
    "            # shuffle the datasets for training (if shuffle parameter is set)\n",
    "            shuffle_in_unison(result[\"X_train\"], result[\"y_train\"])\n",
    "            shuffle_in_unison(result[\"X_test\"], result[\"y_test\"])\n",
    "    else:    \n",
    "        # split the dataset randomly\n",
    "        result[\"X_train\"], result[\"X_test\"], result[\"y_train\"], result[\"y_test\"] = train_test_split(X, y, \n",
    "                                                                                test_size=test_size, shuffle=shuffle)\n",
    "    # get the list of test set dates\n",
    "    dates = result[\"X_test\"][:, -1, -1]\n",
    "    # retrieve test features from the original dataframe\n",
    "    result[\"test_df\"] = result[\"df\"].loc[dates]\n",
    "    # remove duplicated dates in the testing dataframe\n",
    "    result[\"test_df\"] = result[\"test_df\"][~result[\"test_df\"].index.duplicated(keep='first')]\n",
    "    # remove dates from the training/testing sets & convert to float32\n",
    "    result[\"X_train\"] = result[\"X_train\"][:, :, :len(feature_columns)].astype(np.float32)\n",
    "    result[\"X_test\"] = result[\"X_test\"][:, :, :len(feature_columns)].astype(np.float32)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fef2812",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(sequence_length, n_features, units=256, cell=LSTM, n_layers=2, dropout=0.3,\n",
    "                loss=\"mean_absolute_error\", optimizer=\"rmsprop\", bidirectional=False):\n",
    "    model = Sequential()\n",
    "    for i in range(n_layers):\n",
    "        if i == 0:\n",
    "            # first layer\n",
    "            if bidirectional:\n",
    "                model.add(Bidirectional(cell(units, return_sequences=True), batch_input_shape=(None, sequence_length, n_features)))\n",
    "            else:\n",
    "                model.add(cell(units, return_sequences=True, batch_input_shape=(None, sequence_length, n_features)))\n",
    "        elif i == n_layers - 1:\n",
    "            # last layer\n",
    "            if bidirectional:\n",
    "                model.add(Bidirectional(cell(units, return_sequences=False)))\n",
    "            else:\n",
    "                model.add(cell(units, return_sequences=False))\n",
    "        else:\n",
    "            # hidden layers\n",
    "            if bidirectional:\n",
    "                model.add(Bidirectional(cell(units, return_sequences=True)))\n",
    "            else:\n",
    "                model.add(cell(units, return_sequences=True))\n",
    "        # add dropout after each layer\n",
    "        model.add(Dropout(dropout))\n",
    "    model.add(Dense(1, activation=\"linear\"))\n",
    "    model.compile(loss=loss, metrics=[\"mean_absolute_error\"], optimizer=optimizer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6334619d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Sequential()\n",
    "# cell=LSTM\n",
    "# model.add(cell(256, return_sequences=True, batch_input_shape=(None, 50, 5)))\n",
    "# # print(type(cell(256, return_sequences=True, batch_input_shape=(None, 50, 5))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dacedfb9",
   "metadata": {},
   "source": [
    "### Model creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787426f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from tensorflow.keras.layers import LSTM\n",
    "\n",
    "# Window size or the sequence length\n",
    "N_STEPS = 50\n",
    "# Lookup step, 1 is the next day\n",
    "LOOKUP_STEP = 15\n",
    "# whether to scale feature columns & output price as well\n",
    "SCALE = True\n",
    "scale_str = f\"sc-{int(SCALE)}\"\n",
    "# whether to shuffle the dataset\n",
    "SHUFFLE = True\n",
    "shuffle_str = f\"sh-{int(SHUFFLE)}\"\n",
    "# whether to split the training/testing set by date\n",
    "SPLIT_BY_DATE = False\n",
    "split_by_date_str = f\"sbd-{int(SPLIT_BY_DATE)}\"\n",
    "# test ratio size, 0.2 is 20%\n",
    "TEST_SIZE = 0.2\n",
    "# features to use\n",
    "FEATURE_COLUMNS = [\"adjclose\", \"volume\", \"open\", \"high\", \"low\"]\n",
    "# date now\n",
    "date_now = time.strftime(\"%Y-%m-%d\")\n",
    "### model parameters\n",
    "N_LAYERS = 2\n",
    "# LSTM cell\n",
    "CELL = LSTM\n",
    "# 256 LSTM neurons\n",
    "UNITS = 256\n",
    "# 40% dropout\n",
    "DROPOUT = 0.4\n",
    "# whether to use bidirectional RNNs\n",
    "BIDIRECTIONAL = False\n",
    "### training parameters\n",
    "# mean absolute error loss\n",
    "# LOSS = \"mae\"\n",
    "# huber loss\n",
    "LOSS = \"huber_loss\"\n",
    "OPTIMIZER = \"adam\"\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 500\n",
    "# Amazon stock market\n",
    "ticker = \"AMZN\"\n",
    "ticker_data_filename = os.path.join(\"data\", f\"{ticker}_{date_now}.csv\")\n",
    "# model name to save, making it as unique as possible based on parameters\n",
    "model_name = f\"{date_now}_{ticker}-{shuffle_str}-{scale_str}-{split_by_date_str}-\\\n",
    "{LOSS}-{OPTIMIZER}-{CELL.__name__}-seq-{N_STEPS}-step-{LOOKUP_STEP}-layers-{N_LAYERS}-units-{UNITS}\"\n",
    "if BIDIRECTIONAL:\n",
    "    model_name += \"-b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63eea24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create these folders if they does not exist\n",
    "if not os.path.isdir(\"results\"):\n",
    "    os.mkdir(\"results\")\n",
    "if not os.path.isdir(\"logs\"):\n",
    "    os.mkdir(\"logs\")\n",
    "if not os.path.isdir(\"data\"):\n",
    "    os.mkdir(\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2bbf85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "data = load_data(ticker, N_STEPS, scale=SCALE, split_by_date=SPLIT_BY_DATE, \n",
    "                shuffle=SHUFFLE, lookup_step=LOOKUP_STEP, test_size=TEST_SIZE, \n",
    "                feature_columns=FEATURE_COLUMNS)\n",
    "# save the dataframe\n",
    "data[\"df\"].to_csv(ticker_data_filename)\n",
    "# construct the model\n",
    "model = create_model(N_STEPS, len(FEATURE_COLUMNS), loss=LOSS, units=UNITS, cell=CELL, n_layers=N_LAYERS,\n",
    "                    dropout=DROPOUT, optimizer=OPTIMIZER, bidirectional=BIDIRECTIONAL)\n",
    "# some tensorflow callbacks\n",
    "checkpointer = ModelCheckpoint(os.path.join(\"results\", model_name + \".h5\"), save_weights_only=True, save_best_only=True, verbose=1)\n",
    "tensorboard = TensorBoard(log_dir=os.path.join(\"logs\", model_name))\n",
    "# train the model and save the weights whenever we see \n",
    "# a new optimal model using ModelCheckpoint\n",
    "history = model.fit(data[\"X_train\"], data[\"y_train\"],\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    epochs=EPOCHS,\n",
    "                    validation_data=(data[\"X_test\"], data[\"y_test\"]),\n",
    "                    callbacks=[checkpointer, tensorboard],\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa82adcb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2fcdae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
